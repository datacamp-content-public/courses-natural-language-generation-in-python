---
key: a05d49024f4aed8f2357b773c2862262
title: 'Insert title here'
---

## Define the model for text autocomplete

```yaml
type: TitleSlide
key: efac3e9cbc
```

`@lower_third`
name: Biswanath Halder
title: Data Science Manager, Adobe

`@script`
In the last set of exercises, you preprocessed the raw texts, created the training sequences and the corresponding one-hot encoded output word for each of those sequences. Now, let's build a deep LSTM network which will be trained on this training data and will be used to predict the next word given a sequence of words.

---

## Define the model

```yaml
type: FullSlide
key: c2fe7b5451
disable_transition: false
hide_title: false
code_zoom: 110
```

`@part1`
Do the necessary imports {{1}}
```r
import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding
``` {{2}}

Create an object of Sequential model {{3}}
```r
model = Sequential()
``` {{4}}

`@script`
First let's do the necessary imports. We'll import the sequential model from Keras. We'll also import the Embedding, LSTM and the Dense layer. Next we'll start building the network by instantiating the sequential model. The sequential approach helps us to easily stack layers into our network without worrying too much about all the tensors and their shapes flowing through the model.

---

## A deeper network

```yaml
type: FullSlide
key: 02b1d87925
code_zoom: 110
```

`@part1`
Add an embedding layer {{1}}
```r
model.add(Embedding(vocabulary_size, output_dim, 
                    input_length=seq_len))
``` {{2}}
Add the LSTM layers {{3}}
```r
model.add(LSTM(150, return_sequences=True))
model.add(LSTM(150))
``` {{4}}

`@script`
The next step is to add a word embedding layer which will convert our words into meaningful dense embedding vectors. The first argument to the embedding layer is the size of the vocabulary and the second argument is the resultant embedding vector. Also, as this is the first layer of the network, we need to specify the length of the input which is the length of each of the sequences. Next, we add our first LSTM layer for which we need to specify the number of nodes in the hidden layer within the LSTM cell. We also need to set return_sequences to True which will decide whether to return the last output in the output sequence, or the full sequence. We then continue to add another LSTM layer.

---

## Let's practice!

```yaml
type: FinalSlide
key: a323810a3c
```

`@script`
